{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e16ca6",
   "metadata": {},
   "source": [
    "# Batch Processing\n",
    "\n",
    "Here, we will use a jupyter notebook in a non-traditional way just to keep the workshop consistent. We are going to run our code in a single cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfb59f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8beb124ab7a4c35b93cf63c04b14dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=71)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_233265/3637747103.py\u001b[0m in \u001b[0;36m<cell line: 246>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparticipant\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEEGFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m#EEG Features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEEGFeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparticipant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEEGFeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparticipant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "'''\n",
    "0. User Input\n",
    "\n",
    "This script now only processes data for one of the two hypotheses at a time. Remember, these hypotheses are as follows:\n",
    "Hypothesis 1: T1 vs T2\n",
    "Hypothesis 2: T1 vs T3\n",
    "The reason for this is that some participants have data for one of the hypotheses, but not the other and adapting this code to account \n",
    "for these differences would make it more complex than I want it to be. As such, designate which hypothesis you want to process below.\n",
    "'''\n",
    "\n",
    "hypothesis = 1 #Either 1 or 2\n",
    "\n",
    "'''\n",
    "1. Setting Up Environment\n",
    "\n",
    "We will begin by setting up our environment and finding the files to process.\n",
    "'''\n",
    "\n",
    "'''\n",
    "1.1. Import Modules\n",
    "\n",
    "First, we must import the modules we will use in this tutorial.\n",
    "'''\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os #For directory information extraction\n",
    "import numpy as np #For basic computations\n",
    "import mne #Main EEG analysis package\n",
    "import pymatreader #MNE depencency for EEGLab files\n",
    "import scipy.fft #To conduct FFT\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "'''\n",
    "1.2. Defining Extraction Function\n",
    "\n",
    "The following function is going to be the code that extracts the EEG features to be used as predictors in your model. \n",
    "I wanted to pull this function out from the main loop below to be explicit that this function is where you do your main modifications. \n",
    "In this function, I conduct FFT analyses as an example of what you can do, but the intent is for you to replace this with the analyses you are interested in.\n",
    "\n",
    "'''\n",
    "\n",
    "def featureExtraction(EEG):\n",
    "    '''\n",
    "    Input:\n",
    "        EEG: An MNE EEG structure. This assumes pre-processed data, but might work with raw data (untested)\n",
    "        \n",
    "    Output:\n",
    "        timeFeatures: A numpy array of size n, depending on the number of features you want to extract\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    1.2.1. Determining Our Frequency Resolution\n",
    "    \n",
    "    The output of our FFT will ultimately be linked to different frequencies. Although it may be a bit abstract right now, \n",
    "    we will compute our frequency resolution ahead of running the FFT as it will make a few of the FFT transformation steps below easier. \n",
    "    We will determine the number of datapoints being transformed into FFTs, determine our frequency resolution, \n",
    "    and then create an array of frequencies, which will correspond to our FFT output.\n",
    "    '''\n",
    "\n",
    "    numberDataPoints = EEG.get_data().shape[2] #Determine how many datapoints will be transformed per trial and channel\n",
    "    frequencyResolution = EEG.info['sfreq']/numberDataPoints #Determine frequency resolution\n",
    "    fftFrequencies = np.arange(frequencyResolution,(EEG.info['sfreq']/2),frequencyResolution) #Determine array of frequencies\n",
    "\n",
    "    '''\n",
    "    1.2.2. FFT Processing Across Trials and Electrodes\n",
    "    \n",
    "    Here, we will loop through our trials and channels for the current file, computing a FFT each time.\n",
    "    '''\n",
    "\n",
    "    FFT = np.zeros((EEG.get_data().shape[0],EEG.get_data().shape[1],int(EEG.get_data().shape[2]/2)-1)) #Create empty array\n",
    "    for trialIndex in range(EEG.get_data().shape[0]): #Cycle through\n",
    "        for channelIndex in range(EEG.get_data().shape[1]): #Cycle through channels\n",
    "            fftOutput = None #Empty variable\n",
    "            fftOutput = scipy.fft.fft(EEG.get_data()[trialIndex,channelIndex,:]) #Compute the Fourier transform\n",
    "            fftOutput = fftOutput/numberDataPoints #Normalize output\n",
    "            fftOutput = np.abs(fftOutput) #Absolute transformation\n",
    "            fftOutput = fftOutput[range(int(numberDataPoints/2))] #Extract the one-sided spectrum\n",
    "            fftOutput = fftOutput*2 #Double values to account for lost values         \n",
    "            fftOutput = fftOutput**2 #Convert to power\n",
    "            fftOutput = fftOutput[1::] #Remove DC Offset\n",
    "            FFT[trialIndex,channelIndex,:] = fftOutput #Assign fft to dataframe after converting for power and multiplying by two to account for the lost negative aspect\n",
    "\n",
    "    '''\n",
    "    1.2.3. Extracting Depression Biomarkers\n",
    "    \n",
    "    Now that we have computed all of the FFTs, we can use our results and extract EEG biomarkers of depression. \n",
    "    In this workshop, we will focus on the Alpha frequency band. We will extract four biomarkers: Frontal Alpha Amplitude, \n",
    "    Parietal Alpha Amplitude, Frontal Alpha Assymetry, and Parietal Alpha Assymetry.\n",
    "    Let's first define the criteria to be used to extract our biomarkers.\n",
    "    '''\n",
    "    \n",
    "    #Alpha band criteria\n",
    "    if hypothesis == 1:\n",
    "        alphaBand = [8,13.5]\n",
    "        alphaIndex = [np.where(alphaBand[0]==fftFrequencies)[0][0], np.where(alphaBand[1]==fftFrequencies)[0][0]]  \n",
    "    else:\n",
    "        alphaBand = [10,13.5]\n",
    "        alphaIndex = [np.where(alphaBand[0]==fftFrequencies)[0][0], np.where(alphaBand[1]==fftFrequencies)[0][0]]\n",
    "\n",
    "    #Electrode criteria\n",
    "    electrodes = np.array(['F7','F8','P7','P8'])\n",
    "    electrodeIndex = []\n",
    "    for currentElectrode in range(len(electrodes)):\n",
    "        electrodeIndex.append(np.where(electrodes[currentElectrode]==np.array(EEG.ch_names))[0][0])\n",
    "        \n",
    "    '''\n",
    "    1.2.4. Extract Four Alpha Quadrants\n",
    "    \n",
    "    We will here extract alpha activity from four quadrants of the scalp to be used in the creation of our biomarkers. \n",
    "    Specifically, we will extract alpha activity in the frontal left, frontal right, parietal left, and parietal right locations of the head. \n",
    "    This code will average the selected electrodes and alpha band, leaving us with alpha power for each trial in each quadrant.\n",
    "    '''\n",
    "\n",
    "    frontalLeft = np.mean(FFT[:,electrodeIndex[0],alphaIndex[0]:alphaIndex[1]+1],axis = (0,1))\n",
    "    frontalRight = np.mean(FFT[:,electrodeIndex[1],alphaIndex[0]:alphaIndex[1]+1],axis = (0,1))\n",
    "    parietalLeft = np.mean(FFT[:,electrodeIndex[2],alphaIndex[0]:alphaIndex[1]+1],axis = (0,1))\n",
    "    parietalRight = np.mean(FFT[:,electrodeIndex[3],alphaIndex[0]:alphaIndex[1]+1],axis = (0,1))\n",
    "          \n",
    "    '''\n",
    "    1.2.5. Compute Biomarkers\n",
    "    \n",
    "    We will now use the four quadrants that we just extracted in different computations to compute our biomarkers. \n",
    "    Again, our biomarkers are frontal alpha amplitude, parietal alpha amplitude, frontal alpha assymetry, \n",
    "    and parietal alpha assumetry. For the amplitude biomarkers, we will average the corresponding quadrants \n",
    "    (e.g., frontal alpha amplitude will require averaging frontal left and frontal right quadrants), \n",
    "    and for the assymetry biomarkers, we will find the difference between the corresponding quadrants \n",
    "    (e.g., frontal alpha assymetry will require subtracting the frontal right from the frontal left quadrants). \n",
    "    '''\n",
    "\n",
    "    frontalAlphaAssymetry = (frontalRight-frontalLeft)/(frontalRight+frontalLeft)\n",
    "    parietalAlphaAssymetry = (parietalRight-parietalLeft)/(parietalRight+parietalLeft)\n",
    "\n",
    "    '''\n",
    "    1.2.6. Concatenate Biomarkers\n",
    "\n",
    "    We have now extracted the EEG features to be used in the next portion of our tutorial, \n",
    "    but still need to package it nicely for use. We will stack the EEG features into a matrix alongside a recording ID \n",
    "    that signifies which recording session the data is from (i.e., T1 - T4). \n",
    "    '''\n",
    "\n",
    "    timeFeatures = np.stack((frontalAlphaAssymetry,parietalAlphaAssymetry))\n",
    "    \n",
    "    return timeFeatures\n",
    "\n",
    "'''\n",
    "1.3. Finding Data Files\n",
    "\n",
    "There are many ways to determine a list of filenames that will be loaded. \n",
    "Here, we opted to simply extract any filename that begins with the tag 0372. \n",
    "This assumes that you are currently in the directory that contains the data.\n",
    "'''\n",
    "\n",
    "#Load outcome data\n",
    "outcomeFile = '/gpfs/data/brainstorm-ws/data/VALIDATION/VALIDATION_Demographics and Clinical Outcomes_All Series.csv'\n",
    "outcomeData = np.asarray(np.genfromtxt(outcomeFile, delimiter=',', skip_header = 1, encoding='utf-8-sig'))\n",
    "outcomeColumns = ('TMSID', 'Series', 'Sex', 'AgeTMSstart', 'SevHxDep', 'outcome')\n",
    "\n",
    "#Extract participant IDs that will be cycled through\n",
    "participantIDs = outcomeData[:,0].astype(int).astype(str)\n",
    "participantIDs = np.char.zfill(participantIDs, 4)\n",
    "participantSeries = outcomeData[:,1].astype(int).astype(str)\n",
    "\n",
    "#Determine where the data lives and the three subfolders (note here we are not considering T4)\n",
    "dataPath = '/gpfs/data/brainstorm-ws/data/VALIDATION/series_'\n",
    "\n",
    "if hypothesis == 1:\n",
    "    folders = ['T1','T2']\n",
    "else:\n",
    "    folders = ['T1','T3']   \n",
    "\n",
    "filenames = [] #Create an empty variable\n",
    "for participantIdx in range(len(participantIDs)): #Cycle through all participants\n",
    "    participantFilenames = []\n",
    "    for currentFolder in folders: #Cycle through the four folders\n",
    "        currentPath = dataPath+participantSeries[participantIdx]+'/'+currentFolder+'/preprocessed/' #Create a path for the specific folder\n",
    "        allFiles = os.listdir(currentPath) #Retrieve a list of all files within the folder\n",
    "        currentFilename = list(filter(lambda f: f.startswith(participantIDs[participantIdx]), allFiles)) #Extract the files for the specific participant \n",
    "        if currentFilename:\n",
    "            participantFilenames.append(currentPath+currentFilename[0]) #Add the filename with path to a list to be used when loading data\n",
    "    \n",
    "    if len(participantFilenames) == 2:\n",
    "        for i in range(2):\n",
    "            filenames.append(participantFilenames[i]) #Add the filename with path to a list to be used when loading data\n",
    "            \n",
    "'''\n",
    "2. The Main loop\n",
    "\n",
    "Here we loop through all of the files to fully process our data, pulling from our function defined from above in step 1.2.\n",
    "'''\n",
    "\n",
    "#Create empty array to append features\n",
    "EEGFeatures = []\n",
    "fileIndex = 0\n",
    "f = IntProgress(min=0, max=(71)) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "for participantIndex in range(int(len(filenames)/2)): \n",
    "    #Progress report\n",
    "    #print('Complete: '+ str(round((((participantIndex+1)/int(len(filenames)/2))*100),2))+ '%')\n",
    "    f.value += 1\n",
    "    \n",
    "    participantFeatures = []\n",
    "    for timeIndex in range(2):\n",
    "        #Load Data\n",
    "        EEG = mne.io.read_epochs_eeglab(filenames[fileIndex], verbose = 0)\n",
    "\n",
    "        #Call extraction function\n",
    "        timeFeatures = featureExtraction(EEG)\n",
    "        \n",
    "        #Concatenate data across time points\n",
    "        participantFeatures.append(timeFeatures)\n",
    "        \n",
    "        #Increase index count\n",
    "        fileIndex += 1        \n",
    "    \n",
    "    TXvsT1 = np.array(participantFeatures[1]) - np.array(participantFeatures[0]) #Here, X = T2 or T3, depending on your hypothesis\n",
    "\n",
    "    currentFeatures = TXvsT1 #Combine outcome and features\n",
    "    EEGFeatures.append(currentFeatures.tolist()) #Add it to the features matrix\n",
    "    \n",
    "'''\n",
    "#3. Save Features as CSV\n",
    "\n",
    "As a final step, we will save our features as a CSV file for future use. \n",
    "'''\n",
    "\n",
    "if hypothesis == 1:\n",
    "    np.savetxt('autoRAFeaturesDataT1vsT2Validation.csv', EEGFeatures, delimiter=\",\", comments = '')\n",
    "else:\n",
    "    np.savetxt('autoRAFeaturesDataT1vsT3Validation.csv', EEGFeatures, delimiter=\",\", comments = '')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "571de5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5858645492327039, 1]\n",
      "[0.43665615488216575, 0]\n",
      "[0.5406396157694324, 1]\n",
      "[0.4883202615579087, 0]\n",
      "[0.5412539166411052, 1]\n",
      "[0.5761980232032482, 1]\n",
      "[0.5817168548754751, 1]\n",
      "[0.5414772087966917, 1]\n",
      "[0.5558441677101995, 1]\n",
      "[0.6816388600069759, 1]\n",
      "[0.5688724750031219, 1]\n",
      "[0.571320318666073, 1]\n",
      "[0.5844738405158239, 1]\n",
      "[0.5287723529681038, 1]\n",
      "[0.5900291749376941, 1]\n",
      "[0.5686275751671016, 1]\n",
      "[0.5611016771988939, 1]\n",
      "[0.6167738532012438, 1]\n",
      "[0.6010406089194337, 1]\n",
      "[0.4784685569709331, 0]\n",
      "[0.5668695568438096, 1]\n",
      "[0.5476723984686444, 1]\n",
      "[0.4991882122422995, 0]\n",
      "[0.4887102451639921, 0]\n",
      "[0.5720225857735233, 1]\n",
      "[0.5484015294654536, 1]\n",
      "[0.6040130406606676, 1]\n",
      "[0.4859967321127811, 0]\n",
      "[0.7188245814583616, 1]\n",
      "[0.568055838241761, 1]\n",
      "[0.5692747746222192, 1]\n",
      "[0.41846788408389857, 0]\n",
      "[0.6292460767372392, 1]\n",
      "[0.6319337643422143, 1]\n",
      "[0.5789614470486762, 1]\n",
      "[0.5274992013397364, 1]\n",
      "[0.6311391696172365, 1]\n",
      "[0.5219830682687752, 1]\n",
      "[0.5786457281634492, 1]\n",
      "[0.5215074262758786, 1]\n",
      "[0.5494657737907169, 1]\n",
      "[0.6022058422803496, 1]\n",
      "[0.5539335919988728, 1]\n",
      "[0.5313506110968872, 1]\n",
      "[0.6199614366339739, 1]\n",
      "[0.5994032585187934, 1]\n",
      "[0.616643091964491, 1]\n",
      "[0.6934521793621091, 1]\n",
      "[0.491279281300541, 0]\n",
      "[0.5397898198919754, 1]\n",
      "[0.5604227886818935, 1]\n",
      "[0.5684146183166177, 1]\n",
      "[0.46702653728314913, 0]\n",
      "[0.4830224990692617, 0]\n",
      "[0.5749029691477298, 1]\n",
      "[0.5161822812790594, 1]\n",
      "[0.5467524540774565, 1]\n",
      "[0.5778374183516714, 1]\n",
      "[0.5265735766619324, 1]\n",
      "[0.5946613632588215, 1]\n",
      "[0.5584710380334104, 1]\n",
      "[0.6839309419287869, 1]\n",
      "[0.5821293781229299, 1]\n",
      "[0.46963234680665444, 0]\n",
      "[0.5580489776228956, 1]\n",
      "[0.6604152546983131, 1]\n",
      "[0.477813941520424, 0]\n",
      "[0.6369253673116432, 1]\n",
      "[0.48061532494104126, 0]\n",
      "[0.5879200047278246, 1]\n",
      "[0.572158650456198, 1]\n",
      "[[4.600e+01 1.000e+00]\n",
      " [1.920e+02 0.000e+00]\n",
      " [2.060e+02 1.000e+00]\n",
      " [2.060e+02 0.000e+00]\n",
      " [2.260e+02 1.000e+00]\n",
      " [2.600e+02 1.000e+00]\n",
      " [2.640e+02 1.000e+00]\n",
      " [2.710e+02 1.000e+00]\n",
      " [2.750e+02 1.000e+00]\n",
      " [2.760e+02 1.000e+00]\n",
      " [2.870e+02 1.000e+00]\n",
      " [2.890e+02 1.000e+00]\n",
      " [2.900e+02 1.000e+00]\n",
      " [2.920e+02 1.000e+00]\n",
      " [2.940e+02 1.000e+00]\n",
      " [2.960e+02 1.000e+00]\n",
      " [3.060e+02 1.000e+00]\n",
      " [3.150e+02 1.000e+00]\n",
      " [3.170e+02 1.000e+00]\n",
      " [3.190e+02 0.000e+00]\n",
      " [3.290e+02 1.000e+00]\n",
      " [3.290e+02 1.000e+00]\n",
      " [3.290e+02 0.000e+00]\n",
      " [3.310e+02 0.000e+00]\n",
      " [3.390e+02 1.000e+00]\n",
      " [3.430e+02 1.000e+00]\n",
      " [3.440e+02 1.000e+00]\n",
      " [3.450e+02 0.000e+00]\n",
      " [3.470e+02 1.000e+00]\n",
      " [3.500e+02 1.000e+00]\n",
      " [3.510e+02 1.000e+00]\n",
      " [3.540e+02 0.000e+00]\n",
      " [3.560e+02 1.000e+00]\n",
      " [3.580e+02 1.000e+00]\n",
      " [3.600e+02 1.000e+00]\n",
      " [3.630e+02 1.000e+00]\n",
      " [3.680e+02 1.000e+00]\n",
      " [3.690e+02 1.000e+00]\n",
      " [3.740e+02 1.000e+00]\n",
      " [3.910e+02 1.000e+00]\n",
      " [3.990e+02 1.000e+00]\n",
      " [4.020e+02 1.000e+00]\n",
      " [4.080e+02 1.000e+00]\n",
      " [4.100e+02 1.000e+00]\n",
      " [4.110e+02 1.000e+00]\n",
      " [4.120e+02 1.000e+00]\n",
      " [4.170e+02 1.000e+00]\n",
      " [4.210e+02 1.000e+00]\n",
      " [4.240e+02 0.000e+00]\n",
      " [4.240e+02 1.000e+00]\n",
      " [4.290e+02 1.000e+00]\n",
      " [4.380e+02 1.000e+00]\n",
      " [4.420e+02 0.000e+00]\n",
      " [4.580e+02 0.000e+00]\n",
      " [4.590e+02 1.000e+00]\n",
      " [4.600e+02 1.000e+00]\n",
      " [4.650e+02 1.000e+00]\n",
      " [4.710e+02 1.000e+00]\n",
      " [4.730e+02 1.000e+00]\n",
      " [4.750e+02 1.000e+00]\n",
      " [4.790e+02 1.000e+00]\n",
      " [4.800e+02 1.000e+00]\n",
      " [4.820e+02 1.000e+00]\n",
      " [4.850e+02 0.000e+00]\n",
      " [4.930e+02 1.000e+00]\n",
      " [4.940e+02 1.000e+00]\n",
      " [4.970e+02 0.000e+00]\n",
      " [5.060e+02 1.000e+00]\n",
      " [5.090e+02 0.000e+00]\n",
      " [5.160e+02 1.000e+00]\n",
      " [9.001e+03 1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "'''\n",
    "#4. Predict Outcomes of Validation Set\n",
    "\n",
    "In addition to the above, we can use our determined equation to predict outcomes of the validation set\n",
    "'''\n",
    "\n",
    "def ReLU(x):\n",
    "    return(np.maximum(0,x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return(1/(1 + np.exp(-x)))\n",
    "    \n",
    "predictedOutcomes = np.zeros((len(EEGFeatures),2))\n",
    "if hypothesis == 1: \n",
    "    for participant in range(len(EEGFeatures)):\n",
    "        #EEG Features\n",
    "        x1 = EEGFeatures[participant][0]\n",
    "        x2 = EEGFeatures[participant][1]\n",
    "        \n",
    "        #Equation\n",
    "        k1 = np.sin(x1) - x2\n",
    "        k2 = 2*sigmoid(-.04) + np.exp(x2)\n",
    "        y1 = (1.25*k1) + (-.02*k2) + 0.22068863\n",
    "        \n",
    "        #Transform\n",
    "        outcome = sigmoid(y1)\n",
    "        \n",
    "        #Allocate to Matrix\n",
    "        print([outcome,round(outcome)])\n",
    "        predictedOutcomes[participant,0] = participantIDs[participant]\n",
    "        predictedOutcomes[participant,1] = round(outcome)\n",
    "        \n",
    "    #Save to CSV\n",
    "    np.savetxt('Williams_T1T2(H1)_PredictedOutcomes.csv', predictedOutcomes, delimiter=\",\", comments = '')\n",
    "    print(predictedOutcomes)\n",
    "\n",
    "else:\n",
    "    for participant in range(len(EEGFeatures)):\n",
    "        #EEG Features\n",
    "        x1 = EEGFeatures[participant][0]\n",
    "        x2 = EEGFeatures[participant][1]\n",
    "        \n",
    "        #Equation\n",
    "        k1 = ReLU(x1) + np.cos(x2)\n",
    "        k2 = ReLU(x1) + np.cos(x2) + np.tanh(k1)\n",
    "        y1 = (-0.24*k1) + (-.16*k2) + 0.659459\n",
    "        \n",
    "        #Transform\n",
    "        outcome = sigmoid(y1)\n",
    "        \n",
    "        #Allocate to Matrix\n",
    "        print(y1)\n",
    "        print(round(y1))\n",
    "        predictedOutcomes[participant,0] = participantIDs[participant]\n",
    "        predictedOutcomes[participant,1] = round(outcome)\n",
    "        \n",
    "    #Save to CSV\n",
    "    np.savetxt('Williams_T1T3(H2)_PredictedOutcomes.csv', predictedOutcomes, delimiter=\",\", comments = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "347ba78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800026662400692\n",
      "0.9800026662400692\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
